<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>selection.api API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>selection.api</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="selection.api.APIQuery"><code class="flex name class">
<span>class <span class="ident">APIQuery</span></span>
<span>(</span><span>model, timeout=30, temperature=0, max_tokens=256, return_logprobs=False, api='openai', chat=True, max_retries=20, requests_per_second=30, check_every_n_seconds=0.1, read_cost=None, write_cost=None, sequential=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes an instance of the API class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>str</code></dt>
<dd>The model to query.</dd>
<dt><strong><code>timeout</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The timeout value in seconds. Defaults to 30.</dd>
<dt><strong><code>temperature</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The temperature value. Defaults to 0.</dd>
<dt><strong><code>max_tokens</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The maximum number of tokens. Defaults to 256.</dd>
<dt><strong><code>return_logprobs</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to return log probabilities. Defaults to False.</dd>
<dt><strong><code>api</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The API to be used, one of "openai", "together", "huggingface", "gemini", "claude". Defaults to 'openai'.</dd>
<dt><strong><code>chat</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to enable chat mode. Defaults to True.</dd>
<dt><strong><code>max_retries</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The maximum number of retries. Defaults to 20.</dd>
<dt><strong><code>requests_per_second</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of requests per second. Defaults to 30.</dd>
<dt><strong><code>check_every_n_seconds</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The interval for checking rate limits. Defaults to 0.1.</dd>
<dt><strong><code>read_cost</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The cost of read operations. Defaults to None.</dd>
<dt><strong><code>write_cost</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The cost of write operations. Defaults to None.</dd>
<dt><strong><code>sequential</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to run queries sequentially. Defaults to False.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments for the API model.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class APIQuery:
    def __init__(self, model, 
                 timeout=30, 
                 temperature=0, 
                 max_tokens=256,
                 return_logprobs=False, 
                 api=&#39;openai&#39;, 
                 chat=True, 
                 max_retries=20,
                 requests_per_second=30, 
                 check_every_n_seconds=0.1,
                 read_cost=None,  
                 write_cost=None, 
                 sequential=False,
                 **kwargs):
        &#34;&#34;&#34;
        Initializes an instance of the API class.
        Args:
            model (str): The model to query.
            timeout (int, optional): The timeout value in seconds. Defaults to 30.
            temperature (int, optional): The temperature value. Defaults to 0.
            max_tokens (int, optional): The maximum number of tokens. Defaults to 256.
            return_logprobs (bool, optional): Whether to return log probabilities. Defaults to False.
            api (str, optional): The API to be used, one of &#34;openai&#34;, &#34;together&#34;, &#34;huggingface&#34;, &#34;gemini&#34;, &#34;claude&#34;. Defaults to &#39;openai&#39;.
            chat (bool, optional): Whether to enable chat mode. Defaults to True.
            max_retries (int, optional): The maximum number of retries. Defaults to 20.
            requests_per_second (int, optional): The number of requests per second. Defaults to 30.
            check_every_n_seconds (float, optional): The interval for checking rate limits. Defaults to 0.1.
            read_cost (float, optional): The cost of read operations. Defaults to None.
            write_cost (float, optional): The cost of write operations. Defaults to None.
            sequential (bool, optional): Whether to run queries sequentially. Defaults to False.
            **kwargs: Additional keyword arguments for the API model.
        Returns:
            None
        &#34;&#34;&#34;
        self.model = model
        self.return_logprobs = return_logprobs
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.kwargs = kwargs
        self.tokenizer = None
        if read_cost is None:
            self.read_cost = 1
            self.write_cost = 1
        else:
            self.read_cost = read_cost
            self.write_cost = write_cost
        self.max_retries = max_retries
        self.sequential = sequential
        self.api = api
        self.chat = chat
        self.timeout = timeout
        self.rate_limiter = InMemoryRateLimiter(requests_per_second=requests_per_second, 
                                                check_every_n_seconds=check_every_n_seconds, 
                                                max_bucket_size=requests_per_second)
        self.initialize_api_model()

    def initialize_api_model(self):
        &#34;&#34;&#34;
        Initializes the API model based on the specified API and chat settings.
        Raises:
            ValueError: If the specified API or chat settings are not supported.
        Returns:
            None
        &#34;&#34;&#34;
        if self.api == &#39;openai&#39; and self.chat:
            self.api_model = ChatOpenAI(model=self.model, max_retries=self.max_retries, 
                                        temperature=self.temperature,
                                        timeout=self.timeout, max_tokens=self.max_tokens, 
                                        rate_limiter=self.rate_limiter, 
                                        api_key=os.getenv(&#39;OPENAI_API_KEY&#39;), seed=0, 
                                        stream_usage=True, **self.kwargs)
        if self.api == &#39;hyperbolic&#39; and self.chat:
            self.api_model = ChatOpenAI(model=self.model, max_retries=self.max_retries, 
                                        temperature=self.temperature,
                                        timeout=self.timeout, max_tokens=self.max_tokens, 
                                        rate_limiter=self.rate_limiter, 
                                        api_key=os.getenv(&#39;HYPERBOLIC_API_KEY&#39;), seed=0,
                                        base_url=&#34;https://api.hyperbolic.xyz/v1&#34;, 
                                        stream_usage=True, **self.kwargs)
        elif self.api == &#39;openai&#39; and not self.chat:
            self.api_model = OpenAICompletion(model=self.model, temperature=self.temperature,
                                              max_tokens=self.max_tokens, 
                                              api_key=os.getenv(&#39;OPENAI_API_KEY&#39;), seed=0, 
                                              **self.kwargs)
        elif self.api == &#39;hyperbolic&#39; and not self.chat:
            self.api_model = OpenAICompletion(model=self.model, temperature=self.temperature,
                                              max_tokens=self.max_tokens, 
                                              api_key=os.getenv(&#39;HYPERBOLIC_API_KEY&#39;), seed=0,
                                              base_url=&#34;https://api.hyperbolic.xyz/v1&#34;, 
                                              **self.kwargs)
        elif self.api == &#39;anthropic&#39; and self.chat:
            self.api_model = ChatAnthropic(model_name=self.model, temperature=self.temperature,
                                           timeout=self.timeout, max_tokens=self.max_tokens, 
                                           api_key=os.getenv(&#39;ANTHROPIC_API_KEY&#39;), 
                                           stream_usage=True, **self.kwargs)
        elif self.api == &#39;anthropic&#39; and not self.chat:
            self.api_model = AnthropicLLMCompletion(model_name=self.model, temperature=self.temperature,
                                                    max_tokens=self.max_tokens, **self.kwargs)
        elif self.api == &#39;together&#39; and self.chat:
            self.api_model = ChatTogether(model=self.model, temperature=self.temperature,
                                          timeout=self.timeout, max_tokens=self.max_tokens, 
                                          api_key=os.getenv(&#39;TOGETHER_API_KEY&#39;), 
                                          **self.kwargs)
        elif self.api == &#39;together&#39; and not self.chat:
            self.api_model = TogetherLLMCompletion(model=self.model, temperature=self.temperature,
                                                    max_tokens=self.max_tokens, **self.kwargs)
        elif self.api == &#39;google&#39; and self.chat:
            self.api_model = ChatGoogleGenerativeAI(model=self.model, temperature=self.temperature,
                                                    timeout=self.timeout, max_tokens=self.max_tokens, 
                                                    api_key=os.getenv(&#39;GOOGLE_API_KEY&#39;), 
                                                    stream_usage=True, **self.kwargs)
        elif self.api == &#39;huggingface&#39;:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model, padding_side=&#39;left&#39;, trust_remote_code=True)
            try:
                model = AutoModelForCausalLM.from_pretrained(self.model, trust_remote_code=True)
            except:
                model = AutoModelForCausalLM.from_pretrained(self.model, trust_remote_code=False, 
                                                             attn_implementation=&#34;eager&#34;)
            device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
            model.generation_config.eos_token_id = self.tokenizer.eos_token_id
            model.config.eos_token_id = self.tokenizer.eos_token_id
            model.to(device)
            kwargs = self.kwargs.copy()
            if &#39;sequential&#39; in kwargs:
                kwargs.pop(&#39;sequential&#39;)
            if &#34;echo&#34; in kwargs:
                kwargs.pop(&#34;echo&#34;)
            if &#34;logprobs&#34; in kwargs:
                kwargs.pop(&#34;logprobs&#34;)
            pipeline_chat = LogProbsTextGenerationPipeline(
                                model=model,
                                tokenizer=self.tokenizer, 
                                device=&#39;cuda&#39;, 
                                max_new_tokens=self.max_tokens,
                                do_sample=True if self.temperature &gt; 0 else False,
                                temperature=self.temperature,
                                return_full_text=False,
                                **kwargs
                            )
            self.api_model = HuggingFacePipelineFixed(pipeline=pipeline_chat)
        else:
            raise ValueError(f&#39;API {self.api} not supported or chat {self.chat} not supported&#39;)
        
    async def run_queries(self, queries):
        &#34;&#34;&#34;
        Run queries against the API model.

        Args:
            queries (list): A list of queries to be executed. 
                            If chat is enabled, each query is a list of tuples, where each tuple contains (&#39;system&#39;, &#39;ai&#39;, &#39;human&#39;) and the query message.
                            If chat is disabled, each query is a string.
        Returns:
            tuple: A tuple containing the outputs of the queries, the detailed cost, and the total cost.
            Total cost is a dictionary containing the input tokens, output tokens, and the total cost.
            Detailed cost is a list of dictionaries containing the same information for each query.

        Raises:
            ValueError: If the query type is not supported.
        &#34;&#34;&#34;
        retry_api_model = self.api_model
        if self.chat:
            queries_converted = []
            if self.api == &#39;huggingface&#39;:
                for query in queries:
                    current_query = []
                    for query_type, query_message in query:
                        if query_type == &#39;system&#39;:
                            current_query.append({
                                &#39;role&#39;: &#39;system&#39;,
                                &#39;content&#39;: query_message
                            })
                        elif query_type == &#39;ai&#39;:
                            current_query.append({
                                &#39;role&#39;: &#39;assistant&#39;,
                                &#39;content&#39;: query_message
                            })
                        elif query_type == &#39;human&#39;:
                            current_query.append({
                                &#39;role&#39;: &#39;user&#39;,
                                &#39;content&#39;: query_message
                            })
                    current_query = self.tokenizer.apply_chat_template(current_query, tokenize=False)
                    queries_converted.append(current_query)
            else:
                for query in queries:
                    current_query = []
                    for query_type, query_message in query:
                        if query_type == &#39;system&#39;:
                            current_query.append(SystemMessage(content=query_message))
                        elif query_type == &#39;ai&#39;:
                            current_query.append(AIMessage(content=query_message))
                        elif query_type == &#39;human&#39;:
                            current_query.append(HumanMessage(content=query_message))
                        else:
                            raise ValueError(f&#39;Query type {query_type} not supported&#39;)
                    queries_converted.append(current_query)
        else:
            queries_converted = queries

        if self.sequential:
            results, times = await self.run_sequential_queries(retry_api_model.abatch,  
                                                               queries_converted)
        else:
            results = await self._run_with_rate_limiting(retry_api_model.abatch, 
                                                     queries_converted)
            times = [0] * len(results)
        results = self.unify_output_format(results)
        cost, detailed_cost = self.get_cost(results)
        detailed_cost = [{&#39;time&#39;: time, **detailed} for time, detailed in zip(times, detailed_cost)]
        outputs = [result[&#39;content&#39;] for result in results]
        if self.return_logprobs:
            logprob_info = self.get_logprobs(results)
            outputs = [(result[&#39;content&#39;], logprob) 
                       for result, logprob in zip(results, logprob_info)]
        return outputs, detailed_cost, cost
    
    def unify_output_format(self, results):
        &#34;&#34;&#34;
        Unifies the output format of the given results across all APIs.

        Args:
            results (list): A list of results.

        Returns:
            list: A list of unified results.

        &#34;&#34;&#34;
        unified_results = []
        for result in results:
            if result is None:
                unified_results.append({
                    &#39;content&#39;: &#39;&#39;,
                    &#39;usage_metadata&#39;: {
                        &#39;input_tokens&#39;: 0,
                        &#39;output_tokens&#39;: 0
                    },
                    &#34;response_metadata&#34;: {
                        &#34;logprobs&#34;: {
                            &#34;tokens&#34;: [],
                            &#34;token_logprobs&#34;: []
                        }
                    }
                })
                continue
            if not isinstance(result, dict):
                result = dict(result)
            if &#39;generation_info&#39; in result:
                result = result[&#39;generation_info&#39;]
            unified_results.append(result)
        return unified_results
    
    def get_logprobs(self, results):
        &#34;&#34;&#34;
        Retrieves the log probabilities from the given results.
        Parameters:
            results (list): A list of results.
        Returns:
            list: A nested list containing the log probabilities for each result.
        Raises:
            None
        &#34;&#34;&#34;
        if self.api == &#39;huggingface&#39;:
            logprob_info = []
            
            for result in results:
                logprob_info.append((
                    result[&#39;tokens&#39;], 
                    result[&#39;logprobs&#39;]
                ))
            logprob_info = [
                [[(token, logprob)] for token, logprob in zip(result[0], result[1])]
                for result in logprob_info
            ]
        if self.api == &#39;together&#39;:
            logprob_info = []
            
            for result in results:
                logprob_info.append((
                    result[&#39;response_metadata&#39;][&#39;logprobs&#39;][&#39;tokens&#39;], 
                    result[&#39;response_metadata&#39;][&#39;logprobs&#39;][&#39;token_logprobs&#39;]
                ))
            logprob_info = [
                [[(token, logprob)] for token, logprob in zip(result[0], result[1])]
                for result in logprob_info
            ]
        elif self.api == &#39;openai&#39;:
            logprob_info = []
            for result in results:
                result_specific_logprob = []
                for token_logprobs in result[&#39;response_metadata&#39;][&#39;logprobs&#39;][&#39;content&#39;]:
                    top_logprobs_token = []
                    for token in token_logprobs[&#39;top_logprobs&#39;]:
                        top_logprobs_token.append((token[&#39;token&#39;], token[&#39;logprob&#39;]))

                    result_specific_logprob.append(top_logprobs_token)
                logprob_info.append(result_specific_logprob)
        return logprob_info

    async def run_sequential_queries(self, func, queries):
        &#34;&#34;&#34;
        Run queries sequentially against the API model.

        Args:
            func (callable): The function to be executed.
            queries (list): A list of queries to be executed
        Returns:
            results and times
        &#34;&#34;&#34;
        results = []
        times = []
        for query in tqdm(queries):
            time_start = time.time()
            for _ in range(self.max_retries):
                try:
                    result = await func([query])
                    break
                except Exception as e:
                    print(&#39;Traceback: &#39;, e)
                    continue
            time_end = time.time()
            results.extend(result)
            times.append(time_end - time_start)
        return results, times

    async def _run_with_rate_limiting(self, func, queries):
        &#34;&#34;&#34;
        Runs the given function with rate limiting.

        Args:
            func (callable): The function to be executed.
            queries (list): The list of queries to be processed.

        Returns:
            list: The results of the function execution.
        &#34;&#34;&#34;
        results = []
        for i in tqdm(range(0, len(queries), self.rate_limiter.max_bucket_size), 
                      desc=&#39;Running queries&#39;):
            batch = queries[i:i + self.rate_limiter.max_bucket_size]
            if self.api != &#39;huggingface&#39;:
                while not self.rate_limiter.acquire():
                    continue
            n_retries = 0
            for i in range(self.max_retries):
                try:
                    results.extend(await func(batch))
                    break
                except Exception as e:
                    print(f&#39;Error: {e}&#39;)
                    n_retries += 1
                    continue
            if n_retries == self.max_retries:
                print(&#39;Failed to get response after max retries&#39;)
                results.extend([None] * len(batch))
        return results

    def get_cost(self, results):
        &#34;&#34;&#34;
        Calculates the cost of the given results.

        Args:
            results (list): A list of results.

        Returns:
            tuple: A tuple containing the total input tokens, total output tokens, total cost, and detailed cost for each result.

        &#34;&#34;&#34;
        input_tokens = 0
        output_tokens = 0
        detailed_cost = []
        for result in results:
            input_tokens += result[&#39;usage_metadata&#39;][&#39;input_tokens&#39;]
            output_tokens += result[&#39;usage_metadata&#39;][&#39;output_tokens&#39;]
            detailed = result[&#39;usage_metadata&#39;]
            detailed[&#39;cost&#39;] = detailed[&#39;input_tokens&#39;] * self.read_cost / 10 ** 6 
            detailed[&#39;cost&#39;] += detailed[&#39;output_tokens&#39;] * self.write_cost / 10 ** 6
            detailed_cost.append(detailed)

        return {
            &#39;input_tokens&#39;: input_tokens,
            &#39;output_tokens&#39;: output_tokens,
            &#39;cost&#39;: input_tokens * self.read_cost / 10 ** 6 + output_tokens * self.write_cost / 10 ** 6
        }, detailed_cost</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="selection.api.APIQuery.get_cost"><code class="name flex">
<span>def <span class="ident">get_cost</span></span>(<span>self, results)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the cost of the given results.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>results</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of results.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>A tuple containing the total input tokens, total output tokens, total cost, and detailed cost for each result.</dd>
</dl></div>
</dd>
<dt id="selection.api.APIQuery.get_logprobs"><code class="name flex">
<span>def <span class="ident">get_logprobs</span></span>(<span>self, results)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the log probabilities from the given results.</p>
<h2 id="parameters">Parameters</h2>
<p>results (list): A list of results.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>A nested list containing the log probabilities for each result.</dd>
</dl>
<h2 id="raises">Raises</h2>
<p>None</p></div>
</dd>
<dt id="selection.api.APIQuery.initialize_api_model"><code class="name flex">
<span>def <span class="ident">initialize_api_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes the API model based on the specified API and chat settings.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the specified API or chat settings are not supported.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
</dd>
<dt id="selection.api.APIQuery.run_queries"><code class="name flex">
<span>async def <span class="ident">run_queries</span></span>(<span>self, queries)</span>
</code></dt>
<dd>
<div class="desc"><p>Run queries against the API model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>queries</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of queries to be executed.
If chat is enabled, each query is a list of tuples, where each tuple contains ('system', 'ai', 'human') and the query message.
If chat is disabled, each query is a string.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>A tuple containing the outputs of the queries, the detailed cost, and the total cost.</dd>
</dl>
<p>Total cost is a dictionary containing the input tokens, output tokens, and the total cost.
Detailed cost is a list of dictionaries containing the same information for each query.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the query type is not supported.</dd>
</dl></div>
</dd>
<dt id="selection.api.APIQuery.run_sequential_queries"><code class="name flex">
<span>async def <span class="ident">run_sequential_queries</span></span>(<span>self, func, queries)</span>
</code></dt>
<dd>
<div class="desc"><p>Run queries sequentially against the API model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>func</code></strong> :&ensp;<code>callable</code></dt>
<dd>The function to be executed.</dd>
<dt><strong><code>queries</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of queries to be executed</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>results and times</p></div>
</dd>
<dt id="selection.api.APIQuery.unify_output_format"><code class="name flex">
<span>def <span class="ident">unify_output_format</span></span>(<span>self, results)</span>
</code></dt>
<dd>
<div class="desc"><p>Unifies the output format of the given results across all APIs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>results</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of results.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>A list of unified results.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="selection.api.LogProbsTextGenerationPipeline"><code class="flex name class">
<span>class <span class="ident">LogProbsTextGenerationPipeline</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Language generation pipeline using any <code>ModelWithLMHead</code>. This pipeline predicts the words that will follow a
specified text prompt. When the underlying model is a conversational model, it can also accept one or more chats,
in which case the pipeline will operate in chat mode and will continue the chat(s) by adding its response(s).
Each chat takes the form of a list of dicts, where each dict contains "role" and "content" keys.</p>
<p>Examples:</p>
<pre><code class="language-python">&gt;&gt;&gt; from transformers import pipeline

&gt;&gt;&gt; generator = pipeline(model=&quot;openai-community/gpt2&quot;)
&gt;&gt;&gt; generator(&quot;I can't believe you did such a &quot;, do_sample=False)
[{'generated_text': &quot;I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I&quot;}]

&gt;&gt;&gt; # These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.
&gt;&gt;&gt; outputs = generator(&quot;My tart needs some&quot;, num_return_sequences=4, return_full_text=False)
</code></pre>
<pre><code class="language-python">&gt;&gt;&gt; from transformers import pipeline

&gt;&gt;&gt; generator = pipeline(model=&quot;HuggingFaceH4/zephyr-7b-beta&quot;)
&gt;&gt;&gt; # Zephyr-beta is a conversational model, so let's pass it a chat instead of a single string
&gt;&gt;&gt; generator([{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the capital of France? Answer in one word.&quot;}], do_sample=False, max_new_tokens=2)
[{'generated_text': [{'role': 'user', 'content': 'What is the capital of France? Answer in one word.'}, {'role': 'assistant', 'content': 'Paris'}]}]
</code></pre>
<p>Learn more about the basics of using a pipeline in the <a href="../pipeline_tutorial">pipeline tutorial</a>. You can pass text
generation parameters to this pipeline to control stopping criteria, decoding strategy, and more. Learn more about
text generation parameters in <a href="../generation_strategies">Text generation strategies</a> and <a href="text_generation">Text
generation</a>.</p>
<p>This language generation pipeline can currently be loaded from [<code>pipeline</code>] using the following task identifier:
<code>"text-generation"</code>.</p>
<p>The models that this pipeline can use are models that have been trained with an autoregressive language modeling
objective. See the list of available <a href="https://huggingface.co/models?filter=text-generation">text completion models</a>
and the list of <a href="https://huggingface.co/models?other=conversational">conversational models</a>
on [huggingface.co/models].</p>
<h2 id="arguments">Arguments</h2>
<p>model ([<code>PreTrainedModel</code>] or [<code>TFPreTrainedModel</code>]):
The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from
[<code>PreTrainedModel</code>] for PyTorch and [<code>TFPreTrainedModel</code>] for TensorFlow.
tokenizer ([<code>PreTrainedTokenizer</code>]):
The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from
[<code>PreTrainedTokenizer</code>].
modelcard (<code>str</code> or [<code>ModelCard</code>], <em>optional</em>):
Model card attributed to the model for this pipeline.
framework (<code>str</code>, <em>optional</em>):
The framework to use, either <code>"pt"</code> for PyTorch or <code>"tf"</code> for TensorFlow. The specified framework must be
installed.</p>
<pre><code>If no framework is specified, will default to the one currently installed. If no framework is specified and
both frameworks are installed, will default to the framework of the &lt;code&gt;model&lt;/code&gt;, or to PyTorch if no model is
provided.
</code></pre>
<p>task (<code>str</code>, defaults to <code>""</code>):
A task-identifier for the pipeline.
num_workers (<code>int</code>, <em>optional</em>, defaults to 8):
When the pipeline will use <em>DataLoader</em> (when passing a dataset, on GPU for a Pytorch model), the number of
workers to be used.
batch_size (<code>int</code>, <em>optional</em>, defaults to 1):
When the pipeline will use <em>DataLoader</em> (when passing a dataset, on GPU for a Pytorch model), the size of
the batch to use, for inference this is not always beneficial, please read <a href="https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching">Batching with
pipelines</a> .
args_parser ([<code>~pipelines.ArgumentHandler</code>], <em>optional</em>):
Reference to the object in charge of parsing supplied pipeline parameters.
device (<code>int</code>, <em>optional</em>, defaults to -1):
Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on
the associated CUDA device id. You can pass native <code>torch.device</code> or a <code>str</code> too
torch_dtype (<code>str</code> or <code>torch.dtype</code>, <em>optional</em>):
Sent directly as <code>model_kwargs</code> (just a simpler shortcut) to use the available precision for this model
(<code>torch.float16</code>, <code>torch.bfloat16</code>, &hellip; or <code>"auto"</code>)
binary_output (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as
the raw output data e.g. text.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogProbsTextGenerationPipeline(TextGenerationPipeline):
    def __call__(self, input_texts, *args, **kwargs):
        &#34;&#34;&#34;
        Process a batch of input texts and return generated text, log probabilities,
        and tokenized representations for both input and output tokens.

        Args:
            input_texts (list): List of input strings to process.

        Returns:
            list[dict]: A list of dictionaries, each containing:
                - `generated_text`: The generated text string.
                - `all_tokens`: Combined list of input and output tokens.
                - `log_probs`: Log probabilities for each token in `all_tokens`.
        &#34;&#34;&#34;
        # Tokenize all inputs for batching
        inputs = self.tokenizer(input_texts, return_tensors=&#34;pt&#34;, padding=True, truncation=True).to(self.device)

        # Forward pass to compute logits for input tokens
        with torch.no_grad():
            input_outputs = self.model(**inputs)
            input_logits = input_outputs.logits

        # Compute log probabilities for input tokens
        input_log_probs = F.log_softmax(input_logits, dim=-1)

        # Generate output tokens in a batched manner
        model_outputs = self.model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            output_scores=True,
            return_dict_in_generate=True,
            **self._forward_params
        )

        # Extract sequences and scores (logits)
        generated_ids = model_outputs.sequences
        logits = model_outputs.scores

        # Process each input in the batch
        results = []
        for batch_idx in range(len(input_texts)):
            input_length = inputs.input_ids[batch_idx].shape[0]

            # Input tokens and their log probabilities
            input_tokens = self.tokenizer.convert_ids_to_tokens(inputs.input_ids[batch_idx].tolist())
            input_log_probs_batch = [0] + [
                input_log_probs[batch_idx, i, token_id].item()
                for i, token_id in enumerate(inputs.input_ids[batch_idx][1:])
            ]

            # Generated tokens and their log probabilities
            generated_tokens = self.tokenizer.convert_ids_to_tokens(generated_ids[batch_idx].tolist())
            generated_log_probs = []
            for i, step_logits in enumerate(logits):
                step_log_probs = F.log_softmax(step_logits, dim=-1)
                token_id = generated_ids[batch_idx, input_length + i]
                token_log_prob = step_log_probs[batch_idx, token_id].item()
                generated_log_probs.append(token_log_prob)

            # Combine input and output tokens and log probabilities
            all_tokens = input_tokens + generated_tokens[input_length:]
            log_probs = input_log_probs_batch + generated_log_probs

            # Decode the generated text
            generated_text = self.tokenizer.decode(generated_ids[batch_idx], skip_special_tokens=True)
            results.append({
                &#34;content&#34;: generated_text,
                &#34;tokens&#34;: all_tokens,
                &#34;logprobs&#34;: log_probs,
                &#34;usage_metadata&#34;: {
                    &#34;input_tokens&#34;: len(input_tokens),
                    &#34;output_tokens&#34;: len(generated_tokens) - len(input_tokens)
                }
            })
        return results</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.pipelines.text_generation.TextGenerationPipeline</li>
<li>transformers.pipelines.base.Pipeline</li>
<li>transformers.pipelines.base._ScikitCompat</li>
<li>abc.ABC</li>
<li>transformers.utils.hub.PushToHubMixin</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="selection" href="index.html">selection</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="selection.api.APIQuery" href="#selection.api.APIQuery">APIQuery</a></code></h4>
<ul class="">
<li><code><a title="selection.api.APIQuery.get_cost" href="#selection.api.APIQuery.get_cost">get_cost</a></code></li>
<li><code><a title="selection.api.APIQuery.get_logprobs" href="#selection.api.APIQuery.get_logprobs">get_logprobs</a></code></li>
<li><code><a title="selection.api.APIQuery.initialize_api_model" href="#selection.api.APIQuery.initialize_api_model">initialize_api_model</a></code></li>
<li><code><a title="selection.api.APIQuery.run_queries" href="#selection.api.APIQuery.run_queries">run_queries</a></code></li>
<li><code><a title="selection.api.APIQuery.run_sequential_queries" href="#selection.api.APIQuery.run_sequential_queries">run_sequential_queries</a></code></li>
<li><code><a title="selection.api.APIQuery.unify_output_format" href="#selection.api.APIQuery.unify_output_format">unify_output_format</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="selection.api.LogProbsTextGenerationPipeline" href="#selection.api.LogProbsTextGenerationPipeline">LogProbsTextGenerationPipeline</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
