<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>selection.api API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>selection.api</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="selection.api.APIQuery"><code class="flex name class">
<span>class <span class="ident">APIQuery</span></span>
<span>(</span><span>model, timeout=30, temperature=0, max_tokens=256, return_logprobs=False, api='openai', chat=True, max_retries=20, requests_per_second=30, check_every_n_seconds=0.1, read_cost=None, write_cost=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes an instance of the API class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>str</code></dt>
<dd>The model to query.</dd>
<dt><strong><code>timeout</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The timeout value in seconds. Defaults to 30.</dd>
<dt><strong><code>temperature</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The temperature value. Defaults to 0.</dd>
<dt><strong><code>max_tokens</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The maximum number of tokens. Defaults to 256.</dd>
<dt><strong><code>return_logprobs</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to return log probabilities. Defaults to False.</dd>
<dt><strong><code>api</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The API to be used, one of "openai", "together", "huggingface", "gemini", "claude". Defaults to 'openai'.</dd>
<dt><strong><code>chat</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to enable chat mode. Defaults to True.</dd>
<dt><strong><code>max_retries</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The maximum number of retries. Defaults to 20.</dd>
<dt><strong><code>requests_per_second</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of requests per second. Defaults to 30.</dd>
<dt><strong><code>check_every_n_seconds</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The interval for checking rate limits. Defaults to 0.1.</dd>
<dt><strong><code>read_cost</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The cost of read operations. Defaults to None.</dd>
<dt><strong><code>write_cost</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The cost of write operations. Defaults to None.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments for the API model.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class APIQuery:
    def __init__(self, model, 
                 timeout=30, 
                 temperature=0, 
                 max_tokens=256,
                 return_logprobs=False, 
                 api=&#39;openai&#39;, 
                 chat=True, 
                 max_retries=20,
                 requests_per_second=30, 
                 check_every_n_seconds=0.1,
                 read_cost=None,  
                 write_cost=None, 
                 **kwargs):
        &#34;&#34;&#34;
        Initializes an instance of the API class.
        Args:
            model (str): The model to query.
            timeout (int, optional): The timeout value in seconds. Defaults to 30.
            temperature (int, optional): The temperature value. Defaults to 0.
            max_tokens (int, optional): The maximum number of tokens. Defaults to 256.
            return_logprobs (bool, optional): Whether to return log probabilities. Defaults to False.
            api (str, optional): The API to be used, one of &#34;openai&#34;, &#34;together&#34;, &#34;huggingface&#34;, &#34;gemini&#34;, &#34;claude&#34;. Defaults to &#39;openai&#39;.
            chat (bool, optional): Whether to enable chat mode. Defaults to True.
            max_retries (int, optional): The maximum number of retries. Defaults to 20.
            requests_per_second (int, optional): The number of requests per second. Defaults to 30.
            check_every_n_seconds (float, optional): The interval for checking rate limits. Defaults to 0.1.
            read_cost (float, optional): The cost of read operations. Defaults to None.
            write_cost (float, optional): The cost of write operations. Defaults to None.
            **kwargs: Additional keyword arguments for the API model.
        Returns:
            None
        &#34;&#34;&#34;
        self.model = model
        self.return_logprobs = return_logprobs
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.kwargs = kwargs
        if read_cost is None:
            self.read_cost = 1
            self.write_cost = 1
        else:
            self.read_cost = read_cost
            self.write_cost = write_cost
        self.max_retries = max_retries
        self.api = api
        self.chat = chat
        self.timeout = timeout
        self.rate_limiter = InMemoryRateLimiter(requests_per_second=requests_per_second, 
                                                check_every_n_seconds=check_every_n_seconds, 
                                                max_bucket_size=requests_per_second)
        self.initialize_api_model()

    def initialize_api_model(self):
        &#34;&#34;&#34;
        Initializes the API model based on the specified API and chat settings.
        Raises:
            ValueError: If the specified API or chat settings are not supported.
        Returns:
            None
        &#34;&#34;&#34;
        if self.api == &#39;openai&#39; and self.chat:
            self.api_model = ChatOpenAI(model=self.model, max_retries=self.max_retries, 
                                        temperature=self.temperature,
                                        timeout=self.timeout, max_tokens=self.max_tokens, 
                                        rate_limiter=self.rate_limiter, 
                                        api_key=os.getenv(&#39;OPENAI_API_KEY&#39;), seed=0, 
                                        stream_usage=True, **self.kwargs)
        elif self.api == &#39;openai&#39; and not self.chat:
            self.api_model = OpenAICompletion(model=self.model, temperature=self.temperature,
                                              max_tokens=self.max_tokens, 
                                              api_key=os.getenv(&#39;OPENAI_API_KEY&#39;), seed=0, 
                                              **self.kwargs)
        elif self.api == &#39;anthropic&#39; and self.chat:
            self.api_model = ChatAnthropic(model_name=self.model, temperature=self.temperature,
                                           timeout=self.timeout, max_tokens=self.max_tokens, 
                                           api_key=os.getenv(&#39;ANTHROPIC_API_KEY&#39;), 
                                           stream_usage=True, **self.kwargs)
        elif self.api == &#39;anthropic&#39; and not self.chat:
            self.api_model = AnthropicLLMCompletion(model_name=self.model, temperature=self.temperature,
                                                    max_tokens=self.max_tokens, **self.kwargs)
        elif self.api == &#39;together&#39; and self.chat:
            self.api_model = ChatTogether(model=self.model, temperature=self.temperature,
                                          timeout=self.timeout, max_tokens=self.max_tokens, 
                                          api_key=os.getenv(&#39;TOGETHER_API_KEY&#39;), 
                                          **self.kwargs)
        elif self.api == &#39;together&#39; and not self.chat:
            self.api_model = TogetherLLMCompletion(model=self.model, temperature=self.temperature,
                                                    max_tokens=self.max_tokens, **self.kwargs)
        elif self.api == &#39;google&#39; and self.chat:
            self.api_model = ChatGoogleGenerativeAI(model=self.model, temperature=self.temperature,
                                                    timeout=self.timeout, max_tokens=self.max_tokens, 
                                                    api_key=os.getenv(&#39;GOOGLE_API_KEY&#39;), 
                                                    stream_usage=True, **self.kwargs)
        else:
            raise ValueError(f&#39;API {self.api} not supported or chat {self.chat} not supported&#39;)
        
    async def run_queries(self, queries):
        &#34;&#34;&#34;
        Run queries against the API model.

        Args:
            queries (list): A list of queries to be executed. 
                            If chat is enabled, each query is a list of tuples, where each tuple contains (&#39;system&#39;, &#39;ai&#39;, &#39;human&#39;) and the query message.
                            If chat is disabled, each query is a string.
        Returns:
            tuple: A tuple containing the outputs of the queries, the detailed cost, and the total cost.
            Total cost is a dictionary containing the input tokens, output tokens, and the total cost.
            Detailed cost is a list of dictionaries containing the same information for each query.

        Raises:
            ValueError: If the query type is not supported.
        &#34;&#34;&#34;
        retry_api_model = self.api_model.with_retry(stop_after_attempt=self.max_retries)
        if self.chat:
            queries_converted = []
            for query in queries:
                current_query = []
                for query_type, query_message in query:
                    if query_type == &#39;system&#39;:
                        current_query.append(SystemMessage(content=query_message))
                    elif query_type == &#39;ai&#39;:
                        current_query.append(AIMessage(content=query_message))
                    elif query_type == &#39;human&#39;:
                        current_query.append(HumanMessage(content=query_message))
                    else:
                        raise ValueError(f&#39;Query type {query_type} not supported&#39;)
                queries_converted.append(current_query)
        else:
            queries_converted = queries

        results = await self._run_with_rate_limiting(retry_api_model.abatch, 
                                                     queries_converted)
        results = self.unify_output_format(results)
        cost, detailed_cost = self.get_cost(results)
        outputs = [result[&#39;content&#39;] for result in results]
        if self.return_logprobs:
            logprob_info = self.get_logprobs(results)
            outputs = [(result[&#39;content&#39;], logprob) 
                       for result, logprob in zip(results, logprob_info)]
        return outputs, detailed_cost, cost
    
    def unify_output_format(self, results):
        &#34;&#34;&#34;
        Unifies the output format of the given results across all APIs.

        Args:
            results (list): A list of results.

        Returns:
            list: A list of unified results.

        &#34;&#34;&#34;
        unified_results = []
        for result in results:
            if not isinstance(result, dict):
                result = dict(result)
            if &#39;generation_info&#39; in result:
                result = result[&#39;generation_info&#39;]
            unified_results.append(result)
        return unified_results
    
    def get_logprobs(self, results):
        &#34;&#34;&#34;
        Retrieves the log probabilities from the given results.
        Parameters:
            results (list): A list of results.
        Returns:
            list: A nested list containing the log probabilities for each result.
        Raises:
            None
        &#34;&#34;&#34;
        if self.api == &#39;huggingface&#39;:
            logprob_info = [
                [[(key, val) for key, val in result[&#39;logprobs&#39;].items()]] for result in results
            ]
        if self.api == &#39;together&#39;:
            logprob_info = []
            
            for result in results:
                logprob_info.append((
                    result[&#39;response_metadata&#39;][&#39;logprobs&#39;][&#39;tokens&#39;], 
                    result[&#39;response_metadata&#39;][&#39;logprobs&#39;][&#39;token_logprobs&#39;]
                ))
            logprob_info = [
                [[(token, logprob)] for token, logprob in zip(result[0], result[1])]
                for result in logprob_info
            ]
        elif self.api == &#39;openai&#39;:
            logprob_info = []
            for result in results:
                result_specific_logprob = []
                for token_logprobs in result[&#39;response_metadata&#39;][&#39;logprobs&#39;][&#39;content&#39;]:
                    top_logprobs_token = []
                    for token in token_logprobs[&#39;top_logprobs&#39;]:
                        top_logprobs_token.append((token[&#39;token&#39;], token[&#39;logprob&#39;]))

                    result_specific_logprob.append(top_logprobs_token)
                logprob_info.append(result_specific_logprob)
        return logprob_info

    
    async def _run_with_rate_limiting(self, func, queries):
        &#34;&#34;&#34;
        Runs the given function with rate limiting.

        Args:
            func (callable): The function to be executed.
            queries (list): The list of queries to be processed.

        Returns:
            list: The results of the function execution.
        &#34;&#34;&#34;
        results = []
        for i in tqdm(range(0, len(queries), self.rate_limiter.max_bucket_size), 
                      desc=&#39;Running queries&#39;):
            batch = queries[i:i + self.rate_limiter.max_bucket_size]
            while not self.rate_limiter.acquire():
                continue
            results.extend(await func(batch))
        return results

    def get_cost(self, results):
        &#34;&#34;&#34;
        Calculates the cost of the given results.

        Args:
            results (list): A list of results.

        Returns:
            tuple: A tuple containing the total input tokens, total output tokens, total cost, and detailed cost for each result.

        &#34;&#34;&#34;
        input_tokens = 0
        output_tokens = 0
        detailed_cost = []
        for result in results:
            input_tokens += result[&#39;usage_metadata&#39;][&#39;input_tokens&#39;]
            output_tokens += result[&#39;usage_metadata&#39;][&#39;output_tokens&#39;]
            detailed = result[&#39;usage_metadata&#39;]
            detailed[&#39;cost&#39;] = detailed[&#39;input_tokens&#39;] * self.read_cost / 10 ** 6 
            detailed[&#39;cost&#39;] += detailed[&#39;output_tokens&#39;] * self.write_cost / 10 ** 6
            detailed_cost.append(detailed)

        return {
            &#39;input_tokens&#39;: input_tokens,
            &#39;output_tokens&#39;: output_tokens,
            &#39;cost&#39;: input_tokens * self.read_cost / 10 ** 6 + output_tokens * self.write_cost / 10 ** 6
        }, detailed_cost</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="selection.api.APIQuery.get_cost"><code class="name flex">
<span>def <span class="ident">get_cost</span></span>(<span>self, results)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the cost of the given results.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>results</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of results.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>A tuple containing the total input tokens, total output tokens, total cost, and detailed cost for each result.</dd>
</dl></div>
</dd>
<dt id="selection.api.APIQuery.get_logprobs"><code class="name flex">
<span>def <span class="ident">get_logprobs</span></span>(<span>self, results)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the log probabilities from the given results.</p>
<h2 id="parameters">Parameters</h2>
<p>results (list): A list of results.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>A nested list containing the log probabilities for each result.</dd>
</dl>
<h2 id="raises">Raises</h2>
<p>None</p></div>
</dd>
<dt id="selection.api.APIQuery.initialize_api_model"><code class="name flex">
<span>def <span class="ident">initialize_api_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes the API model based on the specified API and chat settings.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the specified API or chat settings are not supported.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
</dd>
<dt id="selection.api.APIQuery.run_queries"><code class="name flex">
<span>async def <span class="ident">run_queries</span></span>(<span>self, queries)</span>
</code></dt>
<dd>
<div class="desc"><p>Run queries against the API model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>queries</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of queries to be executed.
If chat is enabled, each query is a list of tuples, where each tuple contains ('system', 'ai', 'human') and the query message.
If chat is disabled, each query is a string.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>A tuple containing the outputs of the queries, the detailed cost, and the total cost.</dd>
</dl>
<p>Total cost is a dictionary containing the input tokens, output tokens, and the total cost.
Detailed cost is a list of dictionaries containing the same information for each query.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the query type is not supported.</dd>
</dl></div>
</dd>
<dt id="selection.api.APIQuery.unify_output_format"><code class="name flex">
<span>def <span class="ident">unify_output_format</span></span>(<span>self, results)</span>
</code></dt>
<dd>
<div class="desc"><p>Unifies the output format of the given results across all APIs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>results</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of results.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>A list of unified results.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="selection" href="index.html">selection</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="selection.api.APIQuery" href="#selection.api.APIQuery">APIQuery</a></code></h4>
<ul class="">
<li><code><a title="selection.api.APIQuery.get_cost" href="#selection.api.APIQuery.get_cost">get_cost</a></code></li>
<li><code><a title="selection.api.APIQuery.get_logprobs" href="#selection.api.APIQuery.get_logprobs">get_logprobs</a></code></li>
<li><code><a title="selection.api.APIQuery.initialize_api_model" href="#selection.api.APIQuery.initialize_api_model">initialize_api_model</a></code></li>
<li><code><a title="selection.api.APIQuery.run_queries" href="#selection.api.APIQuery.run_queries">run_queries</a></code></li>
<li><code><a title="selection.api.APIQuery.unify_output_format" href="#selection.api.APIQuery.unify_output_format">unify_output_format</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
