{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/jdekoninck/miniconda3/envs/selection/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selection import ClassificationCostComputer, ClassificationQualityComputer, CascadeRouter, Router, ConstantStrategy, HyperoptStrategy, BaselineCascader, RepetitiveConstantStrategy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor\n",
    "from loguru import logger\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "# set logger to only show info messages\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, level=\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mmlu_arc_mixeval'\n",
    "data_folder = '../data/classification'\n",
    "\n",
    "train_model_answers = pd.read_json(os.path.join(data_folder, dataset, 'train', 'model_answers.json'))\n",
    "train_costs = pd.read_json(os.path.join(data_folder, dataset, 'train', 'costs.json'))\n",
    "train_qualities = pd.read_json(os.path.join(data_folder, dataset, 'train', 'qualities.json'))\n",
    "train_queries = list(pd.read_json(os.path.join(data_folder, dataset, 'train', 'queries.json'))[0])\n",
    "test_model_answers = pd.read_json(os.path.join(data_folder, dataset, 'test', 'model_answers.json'))\n",
    "test_costs = pd.read_json(os.path.join(data_folder, dataset, 'test', 'costs.json'))\n",
    "test_qualities = pd.read_json(os.path.join(data_folder, dataset, 'test', 'qualities.json'))\n",
    "test_queries = list(pd.read_json(os.path.join(data_folder, dataset, 'test', 'queries.json'))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numpy(model_answers, costs, qualities, models):\n",
    "    model_answers = model_answers[models].values\n",
    "    for i in range(len(model_answers)):\n",
    "        for j in range(len(model_answers[i])):\n",
    "            model_answers[i, j] = np.array(model_answers[i, j])\n",
    "    costs = costs[models].values\n",
    "    qualities = qualities[models].values\n",
    "    return model_answers, costs, qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qualities_averaged = test_qualities.mean(axis=0)\n",
    "test_costs_averaged = test_costs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qualities_averaged = train_qualities.mean(axis=0)\n",
    "train_costs_averaged = train_costs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo      0.599673\n",
       " meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo     0.726307\n",
       " meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo    0.786492\n",
       " google/gemma-2-9b-it                             0.665577\n",
       " google/gemma-2-27b-it                            0.692266\n",
       " google/gemma-2b-it                               0.257625\n",
       " mistralai/Mistral-7B-Instruct-v0.3               0.549564\n",
       " mistralai/Mixtral-8x22B-Instruct-v0.1            0.698529\n",
       " mistralai/Mixtral-8x7B-Instruct-v0.1             0.629085\n",
       " dtype: float64,\n",
       " meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo      0.000048\n",
       " meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo     0.000236\n",
       " meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo    0.001342\n",
       " google/gemma-2-9b-it                             0.000082\n",
       " google/gemma-2-27b-it                            0.000220\n",
       " google/gemma-2b-it                               0.000027\n",
       " mistralai/Mistral-7B-Instruct-v0.3               0.000060\n",
       " mistralai/Mixtral-8x22B-Instruct-v0.1            0.000358\n",
       " mistralai/Mixtral-8x7B-Instruct-v0.1             0.000180\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_qualities_averaged, test_costs_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo      0.610942\n",
       " meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo     0.729760\n",
       " meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo    0.810998\n",
       " google/gemma-2-9b-it                             0.666759\n",
       " google/gemma-2-27b-it                            0.700746\n",
       " google/gemma-2b-it                               0.249240\n",
       " mistralai/Mistral-7B-Instruct-v0.3               0.555678\n",
       " mistralai/Mixtral-8x22B-Instruct-v0.1            0.701299\n",
       " mistralai/Mixtral-8x7B-Instruct-v0.1             0.626416\n",
       " dtype: float64,\n",
       " meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo      0.000048\n",
       " meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo     0.000234\n",
       " meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo    0.001328\n",
       " google/gemma-2-9b-it                             0.000081\n",
       " google/gemma-2-27b-it                            0.000217\n",
       " google/gemma-2b-it                               0.000027\n",
       " mistralai/Mistral-7B-Instruct-v0.3               0.000059\n",
       " mistralai/Mixtral-8x22B-Instruct-v0.1            0.000354\n",
       " mistralai/Mixtral-8x7B-Instruct-v0.1             0.000177\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_qualities_averaged, train_costs_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(cascader, questions, qualities, costs, actual_answers, models, is_router=False):\n",
    "    qualities_output = []\n",
    "    costs_output = []\n",
    "    models_run = []\n",
    "    selected_models = []\n",
    "    for i, question in enumerate(questions):\n",
    "        model_answers = [[None for _ in range(len(qualities[i]))]]\n",
    "        cost = 0\n",
    "        models_run_question = []\n",
    "        for step in range(len(model_answers[0])):\n",
    "            model = cascader.predict([question], model_answers)\n",
    "            if model[0] is None:\n",
    "                break\n",
    "            else:\n",
    "                model_index = models.index(model[0])\n",
    "                models_run_question.append(model_index)\n",
    "                model_answers[0][model_index] = actual_answers[i][model_index]\n",
    "                cost += costs[i][model_index]\n",
    "            if is_router:\n",
    "                break\n",
    "        selected_answer = cascader.select_answer([question], model_answers)\n",
    "        selected_model = models.index(selected_answer[0])\n",
    "        quality = qualities[i][models.index(selected_answer[0])]\n",
    "        qualities_output.append(quality)\n",
    "        costs_output.append(cost)\n",
    "        models_run.append(','.join([str(model) for model in models_run_question]))\n",
    "        selected_models.append(selected_model)\n",
    "    counter = Counter(models_run)\n",
    "    counter_selected = Counter(selected_models)\n",
    "    return {\n",
    "        'quality': np.mean(qualities_output),\n",
    "        'cost': np.mean(costs_output),\n",
    "        'models_run': counter,\n",
    "        'selected_models': counter_selected,\n",
    "        'lambdas': list(cascader.get_lambdas())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [\n",
    "        {\n",
    "            'name': 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',\n",
    "            'huggingface_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
    "            'read_cost': 0.18 * 10 ** -6,\n",
    "            'write_cost': 0.18 * 10 ** -6\n",
    "        },\n",
    "        {\n",
    "            'name': 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',\n",
    "            'huggingface_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct',\n",
    "            'read_cost': 0.88 * 10 ** -6,\n",
    "            'write_cost': 0.88 * 10 ** -6\n",
    "        },\n",
    "        {\n",
    "            'name': 'meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo',\n",
    "            'huggingface_name': 'meta-llama/Meta-Llama-3.1-405B-Instruct',\n",
    "            'read_cost': 0.88 * 10 ** -6,\n",
    "            'write_cost': 0.88 * 10 ** -6\n",
    "        },\n",
    "        {\n",
    "            'name': 'google/gemma-2-9b-it',\n",
    "            'read_cost': 0.3 * 10 ** -6,\n",
    "            'write_cost': 0.3 * 10 ** -6\n",
    "        },\n",
    "        {\n",
    "            'name': 'google/gemma-2-27b-it',\n",
    "            'read_cost': 0.8 * 10 ** -6,\n",
    "            'write_cost': 0.8 * 10 ** -6\n",
    "        },\n",
    "        {\n",
    "            'name': 'google/gemma-2b-it',\n",
    "            'read_cost': 0.1 * 10 ** -6,\n",
    "            'write_cost': 0.1 * 10 ** -6\n",
    "        },\n",
    "        {\n",
    "            'name': 'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "            'read_cost': 0.2 * 10 ** -6,\n",
    "            'write_cost': 0.2 * 10 ** -6\n",
    "        },\n",
    "        {\n",
    "            'name': 'mistralai/Mixtral-8x22B-Instruct-v0.1',\n",
    "            'read_cost': 1.2 * 10 ** -6,\n",
    "            'write_cost': 1.2 * 10 ** -6\n",
    "        },\n",
    "        {\n",
    "            'name': 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "            'read_cost': 0.6 * 10 ** -6,\n",
    "            'write_cost': 0.6 * 10 ** -6\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MinMaxLinearRegression(LinearRegression):\n",
    "    def predict(self, X):\n",
    "        return np.clip(super().predict(X), 0, 1)\n",
    "\n",
    "def log_reg():\n",
    "    return MinMaxLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "quality_computer_baseline = ClassificationQualityComputer(\n",
    "        model_class=lambda: LinearRegression(),\n",
    "        n_highest_include=1,\n",
    "        require_constant_not_run=False,\n",
    "        baseline=True,\n",
    "        include_question_embedding=False,\n",
    "        do_cosine_similarity=False,\n",
    "        is_regression=True,\n",
    "        all_model_combinations=False,\n",
    "        add_entropy=False,\n",
    "        add_equal_argmax=False,\n",
    "        add_js_divergence=False,\n",
    "        include_question_length=True\n",
    "    )\n",
    "quality_computer = ClassificationQualityComputer(\n",
    "        model_class=lambda: LogisticRegression(penalty=None),\n",
    "        n_highest_include=2,\n",
    "        require_constant_not_run=False,\n",
    "        baseline=False,\n",
    "        include_question_embedding=False,\n",
    "        do_cosine_similarity=False,\n",
    "        is_regression=False,\n",
    "        all_model_combinations=True,\n",
    "        add_entropy=False,\n",
    "        add_equal_argmax=True,\n",
    "        add_js_divergence=False,\n",
    "        include_question_length=False,\n",
    "        store_all=True,\n",
    "        include_all_models=False,\n",
    "        lookup_file_name=os.path.join(data_folder, dataset, 'embeddings', 'queries.json'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [all_models[5], all_models[0], all_models[1], all_models[2]]\n",
    "\n",
    "model_names = [model['name'] for model in models]\n",
    "train_model_answers_here, train_costs_here, train_qualities_here = convert_to_numpy(train_model_answers, train_costs, train_qualities, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_answers_here, test_costs_here, test_qualities_here = convert_to_numpy(test_model_answers, test_costs, test_qualities, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 0: 100%|██████████| 16/16 [00:01<00:00, 14.23it/s]\n",
      "Model 1: 100%|██████████| 16/16 [00:01<00:00, 13.26it/s]\n",
      "Model 2: 100%|██████████| 16/16 [00:01<00:00, 14.17it/s]\n",
      "Model 3: 100%|██████████| 16/16 [00:01<00:00, 14.52it/s]\n"
     ]
    }
   ],
   "source": [
    "quality_computer.fit(train_queries[:1800], train_model_answers_here[:1800], train_qualities_here[:1800])\n",
    "quality_computer_baseline.fit(train_queries[:1800], train_model_answers_here[:1800], train_qualities_here[:1800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Time: 0.10339617729187012\n",
      "Model accuracies: [0.7423747276688453, 0.7156862745098039, 0.7263071895424836, 0.7864923747276689]\n",
      "Model losses: [0.18339623804690564, 0.1986865860528536, 0.16339347357586112, 0.14507886760312547]\n",
      "Normal losses: 0.07116637168037891\n",
      "Model accuracies baseline: [0.7423747276688453, 0.5996732026143791, 0.7263071895424836, 0.7864923747276689]\n",
      "Model losses baseline: [0.19127291212781403, 0.24008302469135806, 0.198832153231663, 0.1681873335754055]\n",
      "Normal losses baseline: 0\n",
      "1\n",
      "Time: 0.06497716903686523\n",
      "Model accuracies: [0.7423747276688453, 0.7156862745098039, 0.7263071895424836, 0.7864923747276689]\n",
      "Model losses: [0.1837849222951397, 0.1986865860528536, 0.16339347357586112, 0.14507886760312547]\n",
      "Normal losses: 0.07814294470890375\n",
      "Model accuracies baseline: [0.7423747276688453, 0.5996732026143791, 0.7263071895424836, 0.7864923747276689]\n",
      "Model losses baseline: [0.2576252723311547, 0.24008302469135806, 0.198832153231663, 0.1681873335754055]\n",
      "Normal losses baseline: 0\n",
      "2\n",
      "Time: 0.06489801406860352\n",
      "Model accuracies: [0.8611111111111112, 0.7739651416122004, 0.7363834422657952, 0.7864923747276689]\n",
      "Model losses: [0.11319322802816043, 0.1552580805838591, 0.16306703477620108, 0.144725423652541]\n",
      "Normal losses: 0.1116592383672102\n",
      "Model accuracies baseline: [0.5174291938997821, 0.7723311546840959, 0.7263071895424836, 0.7864923747276689]\n",
      "Model losses baseline: [0.48257080610021785, 0.22766884531590414, 0.198832153231663, 0.1681873335754055]\n",
      "Normal losses baseline: 0\n",
      "3\n",
      "Time: 0.06278300285339355\n",
      "Model accuracies: [0.9104030501089324, 0.8616557734204793, 0.829248366013072, 0.7875816993464052]\n",
      "Model losses: [0.07374350536391495, 0.1088656800511765, 0.11684381017021445, 0.13895338944698932]\n",
      "Normal losses: 0.11795709444135259\n",
      "Model accuracies baseline: [0.4256535947712418, 0.7333877995642701, 0.8311546840958606, 0.7864923747276689]\n",
      "Model losses baseline: [0.5743464052287581, 0.26661220043572986, 0.16884531590413943, 0.1681873335754055]\n",
      "Normal losses baseline: 0\n",
      "4\n",
      "Time: 0.06420350074768066\n",
      "Model accuracies: [0.9275599128540305, 0.8927015250544662, 0.8657407407407407, 0.8545751633986928]\n",
      "Model losses: [0.05865265312647171, 0.08511692832167227, 0.09979997492753703, 0.10418038588542908]\n",
      "Normal losses: nan\n",
      "Model accuracies baseline: [0.3553921568627451, 0.6854575163398693, 0.7930283224400871, 0.8488562091503268]\n",
      "Model losses baseline: [0.6446078431372549, 0.3145424836601307, 0.20697167755991286, 0.1511437908496732]\n",
      "Normal losses baseline: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3971127/3651335510.py:10: RuntimeWarning: divide by zero encountered in divide\n",
      "  W_i = eigvecs @ np.diag(1.0 / np.sqrt(eigvals)) @ eigvecs.T\n",
      "/tmp/ipykernel_3971127/3651335510.py:10: RuntimeWarning: invalid value encountered in matmul\n",
      "  W_i = eigvecs @ np.diag(1.0 / np.sqrt(eigvals)) @ eigvecs.T\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def whiten_samples(samples, covariances):\n",
    "    whitened_samples = []\n",
    "    for x_i, sigma_i in zip(samples, covariances):\n",
    "        # Eigenvalue decomposition\n",
    "        eigvals, eigvecs = np.linalg.eigh(sigma_i)\n",
    "        \n",
    "        # Construct the whitening matrix\n",
    "        W_i = eigvecs @ np.diag(1.0 / np.sqrt(eigvals)) @ eigvecs.T\n",
    "        \n",
    "        # Apply the whitening matrix\n",
    "        y_i = W_i @ x_i\n",
    "        whitened_samples.append(y_i)\n",
    "        \n",
    "    return np.array(whitened_samples)\n",
    "\n",
    "\n",
    "def compute_accuracy_per_model(real, predictions, var_predictions, predictions_all=None):\n",
    "    model_accuracies = []\n",
    "    model_losses = []\n",
    "    normal_losses = []\n",
    "    for model in range(real.shape[1]):\n",
    "        predictions_acc = (predictions[:, model] > 0.5).astype(int)\n",
    "        real_acc = real[:, model]\n",
    "        model_accuracies.append(np.mean(predictions_acc == real_acc))\n",
    "        # model_losses.append(np.mean(np.log(predictions[:, model]) * real[:, model] + np.log(1 - predictions[:, model]) * (1 - real[:, model])))\n",
    "        # absolute error\n",
    "        model_losses.append(np.mean(np.square(predictions[:, model] - real[:, model])))\n",
    "    \n",
    "    if predictions_all is None:\n",
    "        normal_losses = 0\n",
    "    else:\n",
    "        normal_unit_converted = whiten_samples(predictions - predictions_all, var_predictions)\n",
    "        cov = np.cov(normal_unit_converted.T)\n",
    "        # compute distance between cov and identity matrix\n",
    "        normal_losses = np.linalg.norm(cov - np.eye(cov.shape[0]))\n",
    "    return model_accuracies, model_losses, normal_losses\n",
    "\n",
    "for n_answers in range(5):\n",
    "    print(n_answers)\n",
    "    train_here = test_model_answers_here.copy()\n",
    "\n",
    "    t = time.time()\n",
    "    predictions_all, _ = quality_computer.predict(\n",
    "        test_queries, test_model_answers_here\n",
    "    )\n",
    "    print('Time:', time.time() - t)\n",
    "    predictions_all_baseline, _ = quality_computer_baseline.predict(\n",
    "        test_queries, test_model_answers_here\n",
    "    )\n",
    "\n",
    "    for index in range(len(train_here)):\n",
    "        for j in range(len(train_here[index]) - 1, n_answers - 1, -1):\n",
    "            train_here[index][j] = None\n",
    "\n",
    "    predictions, var_predictions = quality_computer.predict(\n",
    "        test_queries, train_here\n",
    "    )\n",
    "\n",
    "    predictions_baseline, var_predictions_baseline = quality_computer_baseline.predict(\n",
    "        test_queries, train_here\n",
    "    )\n",
    "\n",
    "    model_accuracies, model_losses, normal_losses = compute_accuracy_per_model(test_qualities_here, predictions, var_predictions, predictions_all)\n",
    "    print('Model accuracies:', model_accuracies)\n",
    "    print('Model losses:', model_losses)\n",
    "    print('Normal losses:', normal_losses)\n",
    "    model_accuracies_baseline, model_losses_baseline, normal_losses_baseline = compute_accuracy_per_model(test_qualities_here, predictions_baseline, var_predictions_baseline)\n",
    "    print('Model accuracies baseline:', model_accuracies_baseline)\n",
    "    print('Model losses baseline:', model_losses_baseline)\n",
    "    print('Normal losses baseline:', normal_losses_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.0, 1.0, 0.0, 0.25], None)\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(quality_computer.generate_sample_input_output(\n",
    "    test_queries[index], 0, 4, [None, None, None, None]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.06428765,  0.24187216,  0.02706587, -0.03862987]]),\n",
       " array([-0.79534962]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_computer.models[0][''].coef_, quality_computer.models[0][''].intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Question: The cell structure that makes a plant cell more rigid than an animal cell is the\\nA: cell membrane.\\nB: cytoplasm.\\nC: cell wall.\\nD: ribosome.\\n\\nAnswer: C\\n\\nQuestion: Garden plants on Earth require four resources to stay alive: soil, air, water, and sunlight. How many of these resources are necessary for life to exist on the moon or another planet?\\nA: 4\\nB: 3\\nC: 2\\nD: 1\\n\\nAnswer:'] [array([0.29303635, 0.69750749, 0.79207283, 0.85444476])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.29303635202569184,\n",
       " 0.69750748644879,\n",
       " 0.7920728253923097,\n",
       " 0.8544447605214256]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_computer.base_features(\n",
    "    test_queries[index], None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [0 1 1 1]\n",
      " [0 1 1 1]\n",
      " ...\n",
      " [0 1 1 1]\n",
      " [0 1 1 1]\n",
      " [0 0 1 1]]\n",
      "['Question: The cell structure that makes a plant cell more rigid than an animal cell is the\\nA: cell membrane.\\nB: cytoplasm.\\nC: cell wall.\\nD: ribosome.\\n\\nAnswer: C\\n\\nQuestion: Garden plants on Earth require four resources to stay alive: soil, air, water, and sunlight. How many of these resources are necessary for life to exist on the moon or another planet?\\nA: 4\\nB: 3\\nC: 2\\nD: 1\\n\\nAnswer:']\n",
      "[array([0.29303635, 0.69750749, 0.79207283, 0.85444476])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "new_questions = [test_queries[index][0]]\n",
    "question_embedding = quality_computer.compute_sentence_embeddings(new_questions)\n",
    "estimated_qualities = []\n",
    "for question, embedding in zip(new_questions, [question_embedding]):\n",
    "    if not isinstance(question, str):\n",
    "        question = question[0]\n",
    "    cosine_similarity_q = cosine_similarity(embedding.reshape(1, -1), quality_computer.question_embeddings)\n",
    "    estimated_qualities.append(\n",
    "        np.sum(cosine_similarity_q.reshape(-1, 1) * quality_computer.qualities_embeddings, axis=0) / np.sum(cosine_similarity_q)\n",
    "    )\n",
    "    print(quality_computer.qualities_embeddings)\n",
    "print(new_questions)\n",
    "print(estimated_qualities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
